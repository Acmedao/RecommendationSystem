# Next-User Retrieval: Enhancing Cold-Start Recommendations via Generative Next-User Modeling

链接: https://arxiv.org/abs/2506.15267

# What And How

1. 要解决什么问题？ 内容冷启
2. 原先存在什么问题？ 当然是冷启动老生常谈的问题：新内容交互数据太少，曝光和用户的反馈不够
3. 传统方法咋做的？ 从lookalike人群包的角度切入为新内容寻找反馈，但是会面临两个问题：文章中说的粒度有点粗，我的理解是，传统方法一来缺乏对用户动态行为
   和复杂关系的建模能力，只是单纯依赖一些基础特征（如人口统计、兴趣标签）的浅层匹配，二来交互信号利用不足并且多特征融合比较困难
4. 解决手段是什么？主要是通过生成式建模为每个新物料找到可能交互的用户进行推荐，从而提升内容冷启动的效果。个人感觉整体思路很独特，往常我们都是从
   用户的角度来构建物品序列，但这篇文章是从物品的角度出发来构建用户序列。通过transformer来捕捉最近有过互动行为用户之间的单向关系，再利用这些序列来生
   成下一个最有可能与项目互动的用户，同时加入内容的前置embedding信息，我理解应该是综合了item的各类静态属性信息，通过文本emb的这类向量模型产出文中的Prefix Prompt Embeddings

# Some details

1. 对于抖音冷启动的内容有三个阶段可言：无任何交互数据、有限的交互数据、相对充分的交互数据，如果只依赖最近的lookalike用户，没办法很好的衔接处理物品在这三个阶段的冷启，我理解就是需要一种方式能够自适应的去处理着三个阶段
2. 用户序列中可学习[CLS]token构造的作用，文中给出的描述是：弥合序列用户数据和真实请求用户之间的特征域差距。我的理解是这东西就类似于wx+b中的那个b，用这部分来学习拟合两者之间的特征gap
3. 序列数据构建上，将对同一item具有相同正向交互的用户定义为相似用户，也即是这个item的user序列，最大长度为50个id。
4. 网络构建上基于Transformer来捕捉这种用户序列之间的单向关系，生成下一个潜在用户，
   encoder的输入为：$o_1^p, \ldots, o_k^p, o_1^u, \ldots, o_n^u, o_1^{[CLS]} = \text{Encoder}\left(p_1, \ldots, p_k, u_1, \ldots, u_n, [CLS]\right)$
   decoder聚合encoder的输入信息，输入为：$\hat{u}_1, \ldots, \hat{u}_{n+1}, \hat{u}_{\text{next}} = \text{Decoder}\left(q, (o_1^p, \ldots, o_k^p, o_1^u, \ldots, o_n^u, o_1^{[CLS]})\right)$
![img.png](../pic/img.png)  

5. 损失函数loss function由三部分组成分别为：$ \mathcal{L}_{\text{generative}} = \lambda_1 \mathcal{L}_{\text{contrastive}} + \lambda_2 \mathcal{L}_{\text{CE}} + \lambda_3 \mathcal{L}_{\text{auxiliary}} $
   (1). L_contrastive为对比损失，正负样本对分别为，生成的下一用户$\hat{u}_i$的emb和真实交互用户$u_i$的emb应该接近和全局随机采样的用户嵌入$u_j$应该尽可能不相似，该emb后续可直接应用于相似检索服务中

   $$
   \mathcal{L}_{\text{contrastive}} = -\sum_{i: R_{u_i \hat{u}_i} = 1} \log \frac{\exp(f(u_i, \hat{u}_i)/\tau)}{\exp(f(u_i, \hat{u}_i)/\tau) + \sum_{j \neq i} \exp(f(u_j, \hat{u}_i)/\tau)}
   $$

   (2). L_CE为交叉熵损失，正负样本分别为曝光后发生互动的数据和曝光后未发生交互的数据，相比于随机负样本这部分数据更具有信息量，能帮助模型进一步提升个性化能力，避免推荐看似合理但不受欢迎的内容，
   这一点和召回中的负样本构建还有点不太一样，召回中这部分数据一般是不会用的，我理解这部分曝光未互动的数据在抖音视频场景下可能会有用，因为就视频而言用户的决策成本较高，曝光未互动的这部分数据样本就是hard sample

   $$
   \mathcal{L}_{\text{CE}} = -\left(\sum_{i: R_{u_i \hat{u}_i} = 1} \log \sigma(f(u_i, \hat{u}_i)) + \sum_{i: R_{u_i \hat{u}_i} = 0} \log(1 - \sigma(f(u_i, \hat{u}_i)))\right)
   $$

   (3). L_auxiliary为辅助损失，使用均方误差mse的形式，强制生成用户$\hat{u}_j$的emb尽可能接近真实用户$u_j$的emb，sg(⋅) 表示停止梯度（stop-gradient）即计算损失时不对$u_j$进行回传，防止模型崩溃，我理解如果直接优化$|(u_j) - \hat{u}_j \|^2$模型可能会偷懒，
   将所有用户嵌入优化到相同的点，模型的判别能力减弱，停止梯度的作用就在于仅让生成的$\hat{u}_j$向$u_j$靠拢，避免模型走捷径

   $$
   \mathcal{L}_{\text{auxiliary}} = \sum_{i: R_{u_i \hat{u}_i} = 1} \left( \sum_{j=1}^{n+1} \| \text{sg}(u_j) - \hat{u}_j \|^2 \right)
   $$  
6. 实验结果，就不细说了，主要使用recall@Topk来做为实验之间的分析指标，对比传统lookalike不做阉割的模型提升很高，同时也对比了使用prefix emb和更长的序列以及[CLS]token和causal attention对模型带来的影响，
   其中prefix emb和更长的序列对模型影响较大，[CLS]token和causal attention对模型影响较小